{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyAKXCA3yKyq-STvJnsQPcefP54EshjV94M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generative AI model\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to your service account credentials file\n",
    "SERVICE_ACCOUNT_FILE = \"G:\\\\GAURAV\\\\python\\\\gen-lang-client-0832638864-811a793cddac.json\"\n",
    "\n",
    "# Authenticate with Google API using service account\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=[\"https://www.googleapis.com/auth/drive.readonly\"]\n",
    ")\n",
    "\n",
    "# Build the Drive API client\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# Folder ID of the folder containing your resumes\n",
    "FOLDER_ID = '1hmElAZW2syLblzejdkoxs2G7ODjK3gfU'  # Replace with your Google Drive folder ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "resumes_folder_path = \"resumes_folder\"  \n",
    "output_excel = \"output/resumes_batch.xlsx\"  # Path to output Excel file\n",
    "os.makedirs(resumes_folder_path, exist_ok=True)\n",
    "\n",
    "# Ensure output directory exists for saving Excel\n",
    "os.makedirs(os.path.dirname(output_excel), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list all files in the folder\n",
    "def list_files_in_folder(folder_id):\n",
    "    query = f\"'{folder_id}' in parents and trashed = false\"\n",
    "    results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    return results.get('files', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a file using its file ID\n",
    "def download_file(file_id, file_name, download_folder):\n",
    "    file_path = os.path.join(download_folder, file_name)\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    fh = open(file_path, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    \n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(f\"Download {file_name}: {int(status.progress() * 100)}%.\")\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate prompt for accuracy and Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to process resume and extract information using generative AI\n",
    "def process_resume(pdf_path):\n",
    "    # Upload the resume to the generative model\n",
    "    sample_pdf = genai.upload_file(pdf_path, mime_type=\"application/pdf\")\n",
    "    \n",
    "    prompt = (\n",
    "        \"Extract the following details from the document: \"\n",
    "        \"1. Name\\n\"\n",
    "        \"2. Contact details (as in the resume)\\n\"\n",
    "        \"3. University\\n\"\n",
    "        \"4. Year of Study\\n\"\n",
    "        \"5. Course\\n\"\n",
    "        \"6. Discipline\\n\"\n",
    "        \"7. CGPA/Percentage/GPA\\n\"\n",
    "        \"8. Key Skills\\n\"\n",
    "        \"9. Generative AI Experience Score (1-3 scale, where 1 = Exposed, 2 = Hands-on, 3 = Worked on advanced areas such as Agentic RAG, Evals, etc.)\\n\"\n",
    "        \"10. AI/ML Experience Score (1-3 scale, similar to above)\\n\"\n",
    "        \"11. Supporting Information: Divide into categories such as Certifications, Internships, Projects.\"\n",
    "        \"Return the details in a structured JSON format.\"\n",
    "    )\n",
    "\n",
    "    # Get AI model response for resume\n",
    "    response = model.generate_content([prompt, sample_pdf])\n",
    "\n",
    "    if response.candidates and len(response.candidates) > 0:\n",
    "        response_text = response.candidates[0].content.parts[0].text\n",
    "        \n",
    "        # Attempt to parse the extracted content into JSON\n",
    "        start_index = response_text.find('```json')\n",
    "        if start_index != -1:\n",
    "            start_index += len('```json')\n",
    "            end_index = response_text.find('```', start_index)\n",
    "            if end_index != -1:\n",
    "                json_string = response_text[start_index:end_index].strip()\n",
    "                try:\n",
    "                    extracted_data = json.loads(json_string)\n",
    "                    print(f\"Parsed Extracted Data: {extracted_data}\")\n",
    "                    return extracted_data\n",
    "                except json.JSONDecodeError as json_err:\n",
    "                    print(f\"JSON Decode Error: {json_err}. JSON string: {json_string}\") \n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Closing ``` not found in response.\")\n",
    "        else:\n",
    "            print(\"```json not found in response.\")\n",
    "    else:\n",
    "        print(\"No candidates found in response.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten and process supporting information (e.g., certifications, internships, projects)\n",
    "def flatten_data(data):\n",
    "    supporting_info = {\n",
    "        \"Certifications\": data.get(\"Certifications\", \"\"),\n",
    "        \"Internships\": data.get(\"Internships\", \"\"),\n",
    "        \"Projects\": data.get(\"Projects\", \"\")\n",
    "    }\n",
    "    return {**data, **supporting_info}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save extracted data to Excel\n",
    "def save_all_to_excel(all_data, output_file):\n",
    "    if all_data:\n",
    "        # Flatten all data and save to a dataframe\n",
    "        flattened_data = [flatten_data(data) for data in all_data]\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "        print(f\"All data successfully saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main process to download, process and save all resumes in the folder\n",
    "def process_all_resumes():\n",
    "    # List all files in the Google Drive folder\n",
    "    files_in_folder = list_files_in_folder(FOLDER_ID)\n",
    "\n",
    "    all_extracted_data = []\n",
    "\n",
    "    # Download each file and process it\n",
    "    for file in files_in_folder:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        print(f\"Downloading: {file_name}\")\n",
    "        \n",
    "        # Download the resume file to local folder\n",
    "        download_file(file_id, file_name, resumes_folder_path)\n",
    "        \n",
    "        # Process the downloaded resume to extract details\n",
    "        resume_path = os.path.join(resumes_folder_path, file_name)\n",
    "        extracted_data = process_resume(resume_path)\n",
    "\n",
    "        if extracted_data:\n",
    "            all_extracted_data.append(extracted_data)\n",
    "\n",
    "    # Save all extracted data to Excel after processing all resumes\n",
    "    save_all_to_excel(all_extracted_data, output_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: Andrey Kurenkov.pdf\n",
      "Download Andrey Kurenkov.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Andrey Kurenkov', 'Contact Details': {'Phone': '678-900-4326', 'Email': 'andreyvkurenkov@gmail.com', 'Website': 'www.andreykurenkov.com'}, 'University': ['Stanford University', 'Georgia Institute of Technology'], 'Year of Study': {'Stanford University': 'September 2017 - Present', 'Georgia Institute of Technology': 'August 2011 - May 2015'}, 'Course': {'Stanford University': 'M.S. in Computer Science with focus in AI', 'Georgia Institute of Technology': 'Dual major: B.S. in Electrical Engineering, B.S. in Computer Science with Research Option'}, 'Discipline': 'Computer Science', 'CGPA/Percentage/GPA': {'Stanford University': '3.87', 'Georgia Institute of Technology': 'CS 4.0, Overall 3.88'}, 'Key Skills': ['Python', 'Java', 'C', 'C++', 'R', 'MATLAB/Octave', 'LaTeX', 'Numpy', 'Scikit-learn', 'Pandas', 'ROS', 'Tensorflow', 'Docker', 'Deep Learning', 'Robotics', 'Data Science', 'AI'], 'Generative AI Experience Score': 1, 'AI/ML Experience Score': 3, 'Supporting Information': {'Certifications': [], 'Internships': [{'Title': 'Summer Research Intern', 'Organization': 'École Polytechnique Fédérale de Lausanne', 'Dates': 'May 2014 - August 2014'}, {'Title': 'Robotics Institute Summer Scholars Research Intern', 'Organization': 'Carnegie Mellon University', 'Dates': 'June 2013 – August 2013'}], 'Projects': [{'Title': 'Oracle Systems Manager for ZFSSA', 'Description': 'Completed a prototype for time-series data aggregation microservice with Python, Cassandra, Flask, and Docker. Owned design and implementation of REST and logic layers from start of project to beta release (1 year) using Java, Jetty, Jersey, JDBI.', 'Dates': 'June 2015 - July 2017'}, {'Title': 'Georgia Tech Solar Racing Car Team', 'Description': \"Developed high quality telemetry and control software with TI's C2000 Picollo chips.\", 'Dates': 'August 2011 – May 2015'}]}}\n",
      "Downloading: Arun Tejasvi.pdf\n",
      "Download Arun Tejasvi.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Arun Tejasvi Chaganty', 'Contact Details': {'Email': 'arunchaganty@gmail.com', 'Website': 'arun.chagantys.org', 'Github': 'github.com/arunchaganty'}, 'University': 'Stanford University', 'Year of Study': '2012-2018', 'Course': 'PhD (Computer Science)', 'Discipline': 'Computer Science', 'CGPA/Percentage/GPA': '9.24/10', 'Key Skills': ['Natural Language Processing', 'Conversational AI', 'Recommendation Systems', 'Synthetic Data Generation', 'Evaluation', 'Retrieval', 'Crowdsourcing', 'Semantic Parsing', 'Information Extraction', 'Machine Learning', 'Deep Learning', 'Latent Variable Models', 'Probabilistic Programming', 'Python (PyTorch, TensorFlow)', 'Typescript (Angular, React)', 'SQL', 'Bash', 'Java', 'C++', 'Compilers', 'Operating Systems', 'Computer Networks', 'Cloud Computing'], 'Generative AI Experience Score': 3, 'AI/ML Experience Score': 3, 'Supporting Information': {'Certifications': [], 'Internships': [{'Company': 'Microsoft Research India', 'Period': 'Summer 2009, 2010, 2011'}, {'Company': 'Google', 'Period': 'Summer 2014'}], 'Projects': [{'Name': 'Created dialog inpainting', 'Description': 'a technique to generate millions of information-seeking conversations from documents using language models (T5 S-XXL). Implemented the entire bulk inference pipeline (average throughput of ~3k inference calls/s) using Apache Beam. Led human evaluation and safety analysis. Trained masked language models and retrieval models.'}, {'Name': 'Created the Conversational Playlist Curation Dataset', 'Description': '(first author; PI), one of the first resources for conversational recommendation with multiple item ratings per-turn. Designed and implemented human-human methodology, including all annotation interfaces.'}, {'Name': 'Developed Talk the Walk', 'Description': '(PI), a recipe to generate millions of (music) recommendation-seeking conversations from existing playlists using a combination of random walks and language models. Bootstrapped an end-to-end conversation recommendation system that significantly outperforms baselines in live experiments.'}, {'Name': 'Defined task and evaluation methodology for RARR', 'Description': 'a post-hoc attribution and revision method for large language models (PaLM-540B).'}, {'Name': 'Explored multi-sentence relation extraction for knowledge bases', 'Description': ''}, {'Name': 'Square Assistant', 'Description': 'a chatbot that helps customers book and reschedule appointments with Square merchants.'}, {'Name': 'Conversational rescheduling feature', 'Description': 'increased booking and rescheduling success rates by helping customers find a concrete time for their appointment; the feature understands temporal constraints in user utterances using a model-based semantic parser.'}, {'Name': 'Type-safe domain-specific language', 'Description': 'to describe asynchrony and interruptions in dialog flows using coroutines. Implemented Java-to-Java compiler. DSL reduced feature code 10-20x and fixed subtle asynchrony bugs.'}, {'Name': 'Conversational AI system for enterprise customer service', 'Description': 'Interfaced with clients directly.'}, {'Name': 'Human-in-the-loop system to fine-tune question similarity models', 'Description': 'led to 2-3x increases in precision and recall for each client.'}]}}\n",
      "Downloading: ILIA ZAITSEV.pdf\n",
      "Download ILIA ZAITSEV.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'ILIA ZAITSEV', 'Contact Details': '20257, Hamburg, Hamburg Area, Germany\\niliazaitsev.me\\nlinkedin.com/in/ilia-zaitsev\\nmedium.com/@iliazaitsev', 'University': 'Surgut State University, Russia', 'Year of Study': '2009-2014', 'Course': 'Software Engineering', 'Discipline': 'Data Science / Machine Learning Engineering', 'CGPA/Percentage/GPA': 'Not specified', 'Key Skills': ['Machine Learning: PyTorch, Scikit-Learn, TensorFlow, Deep Learning, Computer Vision, Boosted Trees, Statistical Analysis, Model Complexity & Tuning, Regression vs Classification Problems, Reinforcement Learning', 'Data Processing & Visualisation: numpy, pandas, Dask, Jupyter, matplotlib, seaborn, opencv', 'Programming: Object-Oriented & Functional Programming, Python, Rust', 'Cloud Services: AWS SageMaker, EMR, S3, Databricks, Azure, Google Cloud Compute', 'Databases, Backend & Environments: PostgreSQL, SQLAlchemy, Flask, FastAPI, macOS, Linux', 'DevOps: Docker, Jenkins', 'SW Design & IDE Tools: PyCharm, VS Code, pdb, vim', 'Test-driven development: xUnit, pytest'], 'Generative AI Experience Score': 2, 'AI/ML Experience Score': 3, 'Supporting Information': {'Certifications': ['Artificial Intelligence Nanodegree, Udacity', 'Machine Learning Engineer Nanodegree, Udacity', 'CS188.1x Artificial Intelligence / edX, BerkeleyX'], 'Internships': [{'Title': 'Machine Learning Engineer', 'Company': 'Smaato Inc.', 'Dates': 'June 2019 – May 2020'}], 'Projects': ['Kaggle: Generative Dog Images (Competition)', 'Various projects listed under Python Developer (Contractor) and Python Developer & Machine Learning Engineer (Contract)']}}\n",
      "Downloading: Yunlong Jiao.pdf\n",
      "Download Yunlong Jiao.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Yunlong Jiao', 'Contact Details': {'Phone': '+44 7400 724281', 'Email': 'yljiao.ustc@gmail.com', 'LinkedIn': 'linkedin.com/in/yunlong-jiao/'}, 'University': [{'Name': 'Paris Sciences et Lettres – PSL Research University', 'City': 'Paris', 'Country': 'France'}, {'Name': 'University of Paris-Saclay', 'City': 'Orsay', 'Country': 'France'}, {'Name': 'University of Science & Technology of China', 'City': 'Hefei', 'Country': 'China'}, {'Name': 'University of Oxford', 'City': 'Oxford', 'Country': 'UK'}], 'Year of Study': {'PhD': '2013-2017', 'Master': '2012-2013', 'Bachelor': '2008-2012'}, 'Course': {'PhD': 'Doctor of Philosophy', 'Master': 'Master of Science', 'Bachelor': 'Bachelor of Science'}, 'Discipline': ['Computational Biology', 'Mathematics'], 'CGPA/Percentage/GPA': None, 'Key Skills': ['Natural Language Processing', 'Large Language Models', 'Deep Generative Models', 'Neural Text-to-Speech', 'Bias mitigation technologies in NLP', 'Gaussian Processes', 'Time Series Forecasting', 'Multi-modality', 'Kernel Methods', 'Representation Learning', 'Graph Learning', 'Sparsity Regularisation', 'Information Extraction', 'Feature Engineering', 'Large-Scale Unstructured Database', 'Python', 'Deep Learning Frameworks (PyTorch, MXNet)', 'R', 'C/C++', 'Accelerated Computing (CUDA)', 'Cloud Computing (AWS, SageMaker)', 'SQL', 'Bash', 'Git', 'Docker', 'Open Source', 'Unit/Integration Testing', 'CI/CD'], 'Generative AI Experience Score': 3, 'AI/ML Experience Score': 3, 'Supporting Information': {'Certifications': [], 'Internships': [{'Title': 'Data Scientist Intern', 'Company': 'Roche Diagnostics GmbH', 'Dates': 'APR 2015 - JUN 2015'}], 'Projects': []}}\n",
      "Downloading: atticus hawthorn.pdf\n",
      "Download atticus hawthorn.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Atticus Hawthorn', 'Contact details': {'email': 'a.hawthorn@email.com', 'phone': '(123) 456-7890', 'location': 'Santa Monica, CA'}, 'University': 'University of California', 'Year of Study': '2013-2017', 'Course': 'Bachelor of Science', 'Discipline': 'Computer Science', 'CGPA/Percentage/GPA': None, 'Key Skills': ['Pandas', 'Featuretools', 'TensorFlow', 'K-Means', 'NLTK', 'OpenCV', 'Keras', 'GridSearchCV', 'Flask', 'FairML'], 'Generative AI Experience Score': 2, 'AI/ML Experience Score': 3, 'Supporting Information': {'Certifications': [], 'Internships': [{'company': 'Illumina', 'title': 'Machine Learning Intern', 'years': '2016-2017', 'description': ['Automated the data cleaning process for next-generation sequencing (NGS) data sets, saving approximately 21 hours of manual work every month.', 'Used K-Means clustering to process and categorize genomic sequences, resulting in a 32% faster identification of key genetic markers associated with rare diseases.', \"Leveraged NLTK's sentiment analysis to analyze samples of patient feedback on testing kits, achieving a 34% improvement in recognizing customer satisfaction trends.\", 'Perform exploratory data analysis using Pandas on 6.9 million genetic markers to find patterns that led to an 18% increase in detecting high-risk genetic profiles.']}], 'Projects': []}}\n",
      "Downloading: emma davis.pdf\n",
      "Download emma davis.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Emma Davis', 'Contact Details': {'Email': 'e.davis@email.com', 'Phone': '(123) 456-7890', 'Location': 'San Jose, CA'}, 'University': 'Stanford University', 'Year of Study': '2008-2012', 'Course': 'Bachelor of Science', 'Discipline': 'Computer Science', 'CGPA/Percentage/GPA': None, 'Key Skills': ['Python', 'Pandas', 'TensorFlow', 'Apache Hadoop', 'Amazon Redshift', 'AWS', 'NLTK', 'Apache Kafka', 'Git', 'Docker'], 'Generative AI Experience Score': 1, 'AI/ML Experience Score': 2, 'Supporting Information': {'Certifications': [], 'Internships': [], 'Projects': []}}\n",
      "Downloading: gabriel ramirez.pdf\n",
      "Download gabriel ramirez.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Gabriel Ramirez', 'Contact Details': {'email': 'g.ramirez@email.com', 'phone': '(123) 456-7890', 'location': 'Blue Bell, PA', 'linkedin': 'LinkedIn'}, 'University': 'Carnegie Mellon University', 'Year of Study': {'Masters': '2011 - 2013', 'Bachelors': '2007 - 2011'}, 'Course': {'Masters': \"Master's degree\", 'Bachelors': 'Bachelor of Science'}, 'Discipline': 'Statistics', 'CGPA/Percentage/GPA': None, 'Key Skills': ['NumPy', 'Scikit-learn', 'ggplot2', 'dplyr', 'MySQL', 'SQLite', 'Keras', 'PyTorch'], 'Generative AI Experience Score': None, 'AI/ML Experience Score': 3, 'Supporting Information': {'Certifications': ['Certified Machine Learning Engineer (CMLE)'], 'Internships': None, 'Projects': None}}\n",
      "Downloading: elowen graves.pdf\n",
      "Download elowen graves.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Elowen Graves', 'Contact Details': {'email': 'e.graves@email.com', 'phone': '(123) 456-7890', 'location': 'Seattle, WA', 'linkedin': 'LinkedIn'}, 'University': 'University of Washington', 'Year of Study': '2015-2017', 'Course': 'Master of Science', 'Discipline': 'Data Science', 'CGPA/Percentage/GPA': None, 'Key Skills': ['TensorFlow', 'PyTorch', 'Keras', 'Scikit-Learn', 'Apache Spark', 'Jupyter Notebook', 'H2O.ai', 'Microsoft Azure Machine Learning', 'Google Cloud AI', 'IBM Watson'], 'Generative AI Experience Score': 1, 'AI/ML Experience Score': 3, 'Supporting Information': {'Certifications': [], 'Internships': [], 'Projects': [{'title': 'AI/ML Product Manager at Zillow', 'description': 'Implemented AutoML features of Google Cloud AI, streamlined model development, upgraded pricing algorithm using H2O.ai AutoML, led development of machine learning model using TensorFlow, improved image search capabilities'}, {'title': 'Machine Learning Engineer at Expedia Group', 'description': 'Managed integration of Apache Spark Streaming, developed and optimized deep learning models using PyTorch, refined search result relevance algorithm using Scikit-Learn, systematized workflows for analyzing flight delays'}, {'title': 'Junior Machine Learning Engineer at Tableau Software', 'description': 'Overhauled model evaluation pipeline, assisted integration of NLP tools, collaborated with data scientists to preprocess and augment training datasets, coordinated migration of machine learning workflows'}]}}\n",
      "Downloading: kandice Loudor.pdf\n",
      "Download kandice Loudor.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Kandace Loudor', 'Contact Details': {'Email': 'kloudor@email.com', 'Phone': '(123) 456-7890', 'Location': 'Mount Laurel, NJ', 'LinkedIn': 'LinkedIn', 'Github': 'Github'}, 'University': 'Rutgers University', 'Year of Study': 'September 2011 - April 2015', 'Course': 'B.S. Statistics', 'Discipline': 'Statistics', 'CGPA/Percentage/GPA': None, 'Key Skills': ['Python (NumPy, Pandas, Scikit-learn, Keras, Flask)', 'SQL (MySQL, Postgres)', 'Git', 'Time Series Forecasting', 'Productionizing Models', 'Recommendation Engines', 'Customer Segmentation', 'AWS'], 'Generative AI Experience Score': None, 'AI/ML Experience Score': 3, 'Supporting Information': {'Internships': [{'Title': 'Entry-Level Data Analyst', 'Company': 'Avenica', 'Dates': 'April 2015 - March 2016', 'Location': 'Mount Laurel, NJ', 'Description': 'Collaborated with product managers to perform cohort analysis that identified an opportunity to reduce pricing by 21% for a segment of users to boost yearly revenue by $560,000. Constructed operational reporting in Tableau to improve scheduling contractors, saving $90,000 in the annual budget. Implemented a long-term pricing experiment that improved customer lifetime value by 23%. Ran, submitted, and reported on monthly client enrollments, services opted in for, and the employees assigned to clients.'}], 'Projects': [], 'Certifications': []}}\n",
      "Downloading: avani sharma.pdf\n",
      "Download avani sharma.pdf: 100%.\n",
      "Parsed Extracted Data: {'Name': 'Avani Sharma', 'Contact details': 'Mumbai, India 7593890233 avanisharma88@gmail.com', 'University': 'Indian Institute of Technology, Mumbai', 'Year of Study': '2016-2020', 'Course': 'Bachelor of Science — Electrical Engineering and Computer Science', 'Discipline': 'Machine Learning and Cognitive Computing', 'CGPA/Percentage/GPA': '8.5', 'Key Skills': ['Programming: Python, Java, C++, R, MATLAB, Julia', 'Machine Learning: TensorFlow, PyTorch, Keras, Scikit-learn, OpenCV', 'Big Data Technologies: Hadoop, Spark, Hive, Pig, Tableau', 'Cloud Technologies: AWS, GCP, Azure, Docker, Kubernetes'], 'Generative AI Experience Score': 1, 'AI/ML Experience Score': 3, 'Supporting Information': {'Internships': [], 'Projects': [], 'Certifications': []}}\n",
      "All data successfully saved to output/resumes_batch.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Run the process\n",
    "process_all_resumes() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
